llm {
  client = "mlx"  # Options: "ollama", "gemini", "vLLM"

  gemini {
    baseUrl = "https://generativelanguage.googleapis.com/v1beta"
    model = "gemini-2.5-flash"
  }

  ollama {
    baseUrl = "http://172.31.6.131:11434"
    model = "gpt-oss:120b" #"qwen3:235b"
  }

  mlx {
    baseUrl = "http://172.31.9.131:8000"
    model = "mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"
  }

  local {
    baseUrl = "http://192.168.178.59:1234"
    model = "zai-org/glm-4.7-flash"
  }

  vLLM {
    baseUrl = "http://172.31.6.131:8000"
    model = "zai-org/GLM-4.5-Air-FP8"
  }
}

mcp {
  # MCP Server Configuration
  serverUrl = "http://localhost:8081"
}