llm {
  provider = "ollama"  # Options: "ollama", "gemini", vLLM

  gemini {
    model = "gemini-2.0-flash-exp"
  }

  ollama {
    baseUrl = "http://172.31.6.131:11434"
    model = "gpt-oss:120b"
  }

  vLLM {
    baseUrl = "http://172.31.6.131:8000/v1/chat/completions"
    model = "zai-org/GLM-4.5-Air-FP8"
  }
}

mcp {
  # MCP Server Configuration
  serverUrl = "http://localhost:8081/sse"
}