llm {
  client = "ollama"  # Options: "ollama", "gemini", "vLLM"

  gemini {
    baseUrl = "https://generativelanguage.googleapis.com/v1beta"
    model = "gemini-2.5-flash"
  }

  ollama {
    baseUrl = "http://172.31.6.131:11434"
    model = "qwen3-next:latest" #"gpt-oss:120b"
  }

  local {
    baseUrl = "http://192.168.178.59:1234"
    model = "zai-org/glm-4.7-flash"
  }

  vLLM {
    baseUrl = "http://172.31.6.131:8000/v1/chat/completions"
    model = "zai-org/GLM-4.5-Air-FP8"
  }
}

mcp {
  # MCP Server Configuration
  serverUrl = "http://localhost:8081"
}